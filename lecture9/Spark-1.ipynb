{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Mining with Apache Spark\n",
    "\n",
    "In this lecture Spark basics are described.\n",
    "\n",
    "Spark dustribution used in this notebook is of version 2.0.2 and is fetched from http://spark.apache.org/downloads.html\n",
    "\n",
    "Also an appripriate Java installation is needed.\n",
    "\n",
    "**Note:** if you experience some troubles with pyspark freezing on basic operation consider\n",
    "launching jupyter notebook with the following command:\n",
    "```\n",
    "PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --no-browser --port=7777\" pyspark --master 'local[4]' --executor-memory 2000M --driver-memory 2000M\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure the necessary Spark environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, SPARK_HOME + \"/python\")\n",
    "\n",
    "# Add the py4j to the path.\n",
    "# You may need to change the version number to match your install\n",
    "#sys.path.insert(0, os.path.join(SPARK_HOME, 'python/lib/py4j-0.10.3-src.zip'))\n",
    "\n",
    "# Initialize PySpark to predefine the SparkContext variable 'sc'\n",
    "#execfile(os.path.join(SPARK_HOME, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little bit of Functional Programming\n",
    "\n",
    "Functional programming evolved from lambda calculus, an alternative to Turing Machines mathematical framework for defining computations.\n",
    "\n",
    "Generally functional programming provides more abstract notation than imperative, and remarkably well suited for distributed programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High order functions\n",
    "\n",
    "In functional programming everything is a function. Even constants like 1 are (in a little bit tricky manner) defined as functions. There is not any special constructs like `while` or `for` cycles, everything is built around high order functions: functions that can receive or return another functions.\n",
    "\n",
    "In this section we will introduce the most common high-order functions, 95% of everyday programs can be written with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Scala-like notation for function signatures:\n",
    "\n",
    "```scala\n",
    "def f(x: List[Int], f: (Int, String) -> Double): List[Double]\n",
    "```\n",
    "\n",
    "means function that receives two arguments:\n",
    "- `x` of type `Lst[Int]` (list of integers)\n",
    "- `f` function, that receives 2 arguments of types `Int` and `String` and returns `Double`\n",
    "\n",
    "and returns `List[Double]`.\n",
    "\n",
    "Most of the time we will operate with some abstract types and will denote them by capital Latin letters, e.g. `A`, `B`:\n",
    "\n",
    "```scala\n",
    "def f(x: List[A], f: A -> B): List[B]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map\n",
    "\n",
    "```scala\n",
    "def map(f: A -> B, xs: List[A]): List[B]\n",
    "```\n",
    "\n",
    "applies `f` to each element of `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0!', '1!', '2!', '3!', '4!', '5!', '6!', '7!', '8!', '9!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x: str(x) + \"!\", range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Reduce functions\n",
    "\n",
    "```scala\n",
    "def fold(zero: T)(f: (T, T) -> T, xs: List[T]): T\n",
    "def reduce(f: (T, T) -> T, xs: List[T]): T\n",
    "```\n",
    "\n",
    "performs reduce operation, equalent python code:\n",
    "\n",
    "```python\n",
    "acc = zero\n",
    "for x in xs:\n",
    "    acc = f(acc, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min(xs):\n",
    "    return reduce(lambda a, b: a if a < b else b, xs)\n",
    "\n",
    "### python equalent for fold is reduce with third argument\n",
    "def sum(xs):\n",
    "    return reduce(lambda a, b: a + b, xs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "```scala\n",
    "def filter(p: T -> Boolean, xs: List[T]): List[T]\n",
    "```\n",
    "\n",
    "selects elements of `xs` that satisfy predicate `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n",
      "[0, 2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "print filter(lambda x: x % 2 == 0, range(10))\n",
    "print [ x for x in range(10) if x % 2 == 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FlatMap (fmap)\n",
    "\n",
    "```\n",
    "def flatMap(f: T -> List[T], xs: List[T]): List[T]\n",
    "```\n",
    "\n",
    "similar to map, but function may return an orbitrary number of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatMap(f, xs):\n",
    "    return [ y for x in xs for y in f(x) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatMap(lambda x: [x - 1, x], range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter(p, xs):\n",
    "    return flatMap(lambda x: [x] if p(x) else [], xs)\n",
    "\n",
    "def map(f, xs):\n",
    "    return flatMap(lambda x: [f(x)], xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operators are enough to easily construct 90% of everyday programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD\n",
    "\n",
    "Spark operates with a Resilient Distributed Dataset, which is a ... distributed collection of data.\n",
    "RDD can hold a tremendous amount of data, it is limited only by total memory of all machines in cluster.\n",
    "Every Spark program is a transformation of RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an RDD\n",
    "\n",
    "There is a number of ways to create RDD.\n",
    "We will use only two the most simple methods:\n",
    "- `textFile` - loads lines of a text files as RDD, the resulting type is `RDD[String]`;\n",
    "- `parallelize` - make an RDD from a plain collection (e.g. python list).\n",
    "\n",
    "In practice, working with large amounts of data it is rarely possible to make a `RDD` from a collection gathered on a single machine. Usually data is loaded from distributed filesystems (such as Hadoop Distributed FileSystem, HDFS), or from databases (especially, distributed ones, like Cassandra).\n",
    "\n",
    "Usually, such databases provide adapters for Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### all functions for creating a RDD are hold by SparkContext (usually denoted by `sc`).\n",
    "\n",
    "lines = sc.textFile(os.path.join(SPARK_HOME, 'README.md'))\n",
    "numbers = sc.parallelize(xrange(int(1.0e7)))\n",
    "\n",
    "small_numbers = sc.parallelize(xrange(int(1.0e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/opt/spark-2.0.2-bin-hadoop2.7/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " u'supports general computation graphs for data analysis. It also supports a',\n",
       " u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " u'MLlib for machine learning, GraphX for graph processing,',\n",
       " u'and Spark Streaming for stream processing.',\n",
       " u'',\n",
       " u'<http://spark.apache.org/>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and actions\n",
    "\n",
    "There is 2 type of operations one can do with RDDs:\n",
    "- transformations: an operation that transforms RDD into another RDD;\n",
    "- action: an operation that transorms RDD into some plain value.\n",
    "\n",
    "Note that nothing (`void` in `C/C++` and `Java`, `None` in `Python` and `Unit` or `()` in Scala) is also a plain value.\n",
    "\n",
    "The fundamental differences between these is that only action triggers actual computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3.77 ms, total: 3.77 ms\n",
      "Wall time: 1.51 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "plusX = []\n",
    "\n",
    "for x in xrange(100):\n",
    "    ### map is a transformation\n",
    "    ### so no actual computations are performed here\n",
    "    plusX.append(\n",
    "        numbers.map(lambda x: x + 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 480 µs, total: 11.1 ms\n",
      "Wall time: 1.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000005000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### sum is a action\n",
    "### invoking sum triggers materialization of map\n",
    "numbers.map(lambda x: x + 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50 ms, sys: 16.2 ms, total: 66.3 ms\n",
      "Wall time: 66.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49999995000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### note the overhead introduced by net communications and python-java conversions.\n",
    "np.arange(int(1.0e7)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "The most common actions are:\n",
    "- saving dataset to a file (e.g. `saveAsTextFile`);\n",
    "- reduce and its deriviatives (e.g. `sum`, `count`);\n",
    "- sampling (`take`, `collect`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499999500000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_numbers.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_numbers.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations\n",
    "\n",
    "Transformations are RDD to RDD operations.\n",
    "The most common ones are:\n",
    "- map;\n",
    "- filter;\n",
    "- flatMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000500000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_numbers.map(lambda x: x + 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249999500000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_numbers.filter(lambda x: x % 2 == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'a',\n",
       " u'fast',\n",
       " u'and',\n",
       " u'general',\n",
       " u'cluster']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda x: x.split())\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDD transformations\n",
    "\n",
    "There is a special type of RDD: `PairRDD`, which is essentially collection of pairs of values, i.e. key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#', 1),\n",
       " (u'Apache', 1),\n",
       " (u'Spark', 1),\n",
       " (u'Spark', 1),\n",
       " (u'is', 1),\n",
       " (u'a', 1),\n",
       " (u'fast', 1),\n",
       " (u'and', 1),\n",
       " (u'general', 1),\n",
       " (u'cluster', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = words.map(lambda x: (x, 1))\n",
    "\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common operations on `PairRDD` is `groupByKey` and `reduceByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', [1]),\n",
       " (u'R,', [1]),\n",
       " (u'including', [1, 1, 1]),\n",
       " (u'computation', [1]),\n",
       " (u'using:', [1])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.groupByKey().map(lambda (k, v): (k, list(v))).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 1),\n",
       " (u'R,', 1),\n",
       " (u'including', 3),\n",
       " (u'computation', 1),\n",
       " (u'using:', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = counts.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "freqs.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark allows to combine two RDDs. Since data are distributed, keys are natural (and almost only valid) way to tell which value from one dataset corresponds to values from another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Interactive', 11),\n",
       " (u'easiest', 7),\n",
       " (u'stream', 6),\n",
       " (u'runs.', 5),\n",
       " (u'Example', 7),\n",
       " (u'Hadoop-supported', 16),\n",
       " (u'Python,', 7),\n",
       " (u'documentation,', 14),\n",
       " (u'web', 3),\n",
       " (u'fast', 4)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters = words.map(lambda w: (w, len(w))).distinct()\n",
    "n_letters.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'detailed', (8, 2)),\n",
       " (u'engine', (6, 1)),\n",
       " (u'storage', (7, 1)),\n",
       " (u'project', (7, 1)),\n",
       " (u'usage', (5, 1))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters.join(freqs).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'detailed', (8, 2)),\n",
       " (u'engine', (6, 1)),\n",
       " (u'storage', (7, 1)),\n",
       " (u'project', (7, 1)),\n",
       " (u'usage', (5, 1))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters.leftOuterJoin(freqs).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'detailed', (8, 2)),\n",
       " (u'engine', (6, 1)),\n",
       " (u'storage', (7, 1)),\n",
       " (u'project', (7, 1)),\n",
       " (u'usage', (5, 1))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters.rightOuterJoin(freqs).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'detailed', [8], [1, 1]),\n",
       " (u'engine', [6], [1]),\n",
       " (u'storage', [7], [1]),\n",
       " (u'project', [7], [1]),\n",
       " (u'usage', [5], [1])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the most efficient and general join-like transformation\n",
    "### for each unique key it produces pair of iterables with values from each RDD.\n",
    "n_letters.cogroup(counts).map(lambda (k, (v1, v2)): (k, list(v1), list(v2))).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: since operations on data with the same keys are very common (`reduceByKey`) Spark prefers to store all values with the same key on one machine. Thus if a dataset has too much values with the same key, Spark may crush due to memory error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example, word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count = sc.textFile(\n",
    "    os.path.join(SPARK_HOME, 'README.md')\n",
    ").flatMap(\n",
    "    lambda x: x.split()\n",
    ").map(\n",
    "    lambda x: (x, 1)\n",
    ").reduceByKey(\n",
    "    lambda a, b: a + b\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_word_count = word_count.map(lambda (k, v): v).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq = word_count.map(lambda (w, n): (w, float(n) / total_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'when', 0.00211864406779661),\n",
       " (u'R,', 0.00211864406779661),\n",
       " (u'including', 0.006355932203389831),\n",
       " (u'computation', 0.00211864406779661),\n",
       " (u'using:', 0.00211864406779661),\n",
       " (u'guidance', 0.00423728813559322),\n",
       " (u'Scala,', 0.00211864406779661),\n",
       " (u'environment', 0.00211864406779661),\n",
       " (u'only', 0.00211864406779661),\n",
       " (u'rich', 0.00211864406779661),\n",
       " (u'Apache', 0.00211864406779661),\n",
       " (u'sc.parallelize(range(1000)).count()', 0.00211864406779661),\n",
       " (u'Building', 0.00211864406779661),\n",
       " (u'And', 0.00211864406779661),\n",
       " (u'guide,', 0.00211864406779661),\n",
       " (u'return', 0.00423728813559322),\n",
       " (u'Please', 0.006355932203389831),\n",
       " (u'[Eclipse](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-Eclipse)',\n",
       "  0.00211864406779661),\n",
       " (u'Try', 0.00211864406779661),\n",
       " (u'not', 0.00211864406779661),\n",
       " (u'Spark', 0.03177966101694915),\n",
       " (u'scala>', 0.00211864406779661),\n",
       " (u'Note', 0.00211864406779661),\n",
       " (u'cluster.', 0.00211864406779661),\n",
       " (u'./bin/pyspark', 0.00211864406779661),\n",
       " (u'params', 0.00211864406779661),\n",
       " (u'through', 0.00211864406779661),\n",
       " (u'GraphX', 0.00211864406779661),\n",
       " (u'[run', 0.00211864406779661),\n",
       " (u'abbreviated', 0.00211864406779661),\n",
       " (u'For', 0.006355932203389831),\n",
       " (u'##', 0.01694915254237288),\n",
       " (u'library', 0.00211864406779661),\n",
       " (u'see', 0.006355932203389831),\n",
       " (u'\"local\"', 0.00211864406779661),\n",
       " (u'[Apache', 0.00211864406779661),\n",
       " (u'will', 0.00211864406779661),\n",
       " (u'#', 0.00211864406779661),\n",
       " (u'processing,', 0.00211864406779661),\n",
       " (u'for', 0.023305084745762712),\n",
       " (u'[building', 0.00211864406779661),\n",
       " (u'Maven', 0.00211864406779661),\n",
       " (u'[\"Parallel', 0.00211864406779661),\n",
       " (u'provides', 0.00211864406779661),\n",
       " (u'print', 0.00211864406779661),\n",
       " (u'supports', 0.00423728813559322),\n",
       " (u'built,', 0.00211864406779661),\n",
       " (u'[params]`.', 0.00211864406779661),\n",
       " (u'available', 0.00211864406779661),\n",
       " (u'run', 0.014830508474576272),\n",
       " (u'tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).',\n",
       "  0.00211864406779661),\n",
       " (u'This', 0.00423728813559322),\n",
       " (u'Hadoop,', 0.00423728813559322),\n",
       " (u'Tests', 0.00211864406779661),\n",
       " (u'example:', 0.00211864406779661),\n",
       " (u'-DskipTests', 0.00211864406779661),\n",
       " (u'Maven](http://maven.apache.org/).', 0.00211864406779661),\n",
       " (u'thread', 0.00211864406779661),\n",
       " (u'programming', 0.00211864406779661),\n",
       " (u'running', 0.00211864406779661),\n",
       " (u'against', 0.00211864406779661),\n",
       " (u'site,', 0.00211864406779661),\n",
       " (u'comes', 0.00211864406779661),\n",
       " (u'package.', 0.00211864406779661),\n",
       " (u'and', 0.023305084745762712),\n",
       " (u'package.)', 0.00211864406779661),\n",
       " (u'prefer', 0.00211864406779661),\n",
       " (u'documentation,', 0.00211864406779661),\n",
       " (u'submit', 0.00211864406779661),\n",
       " (u'tools', 0.00211864406779661),\n",
       " (u'use', 0.006355932203389831),\n",
       " (u'from', 0.00211864406779661),\n",
       " (u'[project', 0.00423728813559322),\n",
       " (u'./bin/run-example', 0.00423728813559322),\n",
       " (u'fast', 0.00211864406779661),\n",
       " (u'systems.', 0.00211864406779661),\n",
       " (u'<http://spark.apache.org/>', 0.00211864406779661),\n",
       " (u'Hadoop-supported', 0.00211864406779661),\n",
       " (u'way', 0.00211864406779661),\n",
       " (u'README', 0.00211864406779661),\n",
       " (u'MASTER', 0.00211864406779661),\n",
       " (u'engine', 0.00211864406779661),\n",
       " (u'building', 0.00423728813559322),\n",
       " (u'usage', 0.00211864406779661),\n",
       " (u'instance:', 0.00211864406779661),\n",
       " (u'with', 0.00847457627118644),\n",
       " (u'protocols', 0.00211864406779661),\n",
       " (u'IDE,', 0.00211864406779661),\n",
       " (u'this', 0.00211864406779661),\n",
       " (u'setup', 0.00211864406779661),\n",
       " (u'shell:', 0.00423728813559322),\n",
       " (u'project', 0.00211864406779661),\n",
       " (u'following', 0.00423728813559322),\n",
       " (u'distribution', 0.00211864406779661),\n",
       " (u'detailed', 0.00423728813559322),\n",
       " (u'have', 0.00211864406779661),\n",
       " (u'stream', 0.00211864406779661),\n",
       " (u'is', 0.012711864406779662),\n",
       " (u'higher-level', 0.00211864406779661),\n",
       " (u'tests', 0.00423728813559322),\n",
       " (u'1000:', 0.00423728813559322),\n",
       " (u'sample', 0.00211864406779661),\n",
       " (u'[\"Specifying', 0.00211864406779661),\n",
       " (u'Alternatively,', 0.00211864406779661),\n",
       " (u'file', 0.00211864406779661),\n",
       " (u'need', 0.00211864406779661),\n",
       " (u'You', 0.00847457627118644),\n",
       " (u'instructions.', 0.00211864406779661),\n",
       " (u'different', 0.00211864406779661),\n",
       " (u'programs,', 0.00211864406779661),\n",
       " (u'storage', 0.00211864406779661),\n",
       " (u'same', 0.00211864406779661),\n",
       " (u'machine', 0.00211864406779661),\n",
       " (u'Running', 0.00211864406779661),\n",
       " (u'which', 0.00423728813559322),\n",
       " (u'you', 0.00847457627118644),\n",
       " (u'A', 0.00211864406779661),\n",
       " (u'About', 0.00211864406779661),\n",
       " (u'sc.parallelize(1', 0.00211864406779661),\n",
       " (u'locally.', 0.00211864406779661),\n",
       " (u'Hive', 0.00423728813559322),\n",
       " (u'optimized', 0.00211864406779661),\n",
       " (u'uses', 0.00211864406779661),\n",
       " (u'Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       "  0.00211864406779661),\n",
       " (u'variable', 0.00211864406779661),\n",
       " (u'The', 0.00211864406779661),\n",
       " (u'data', 0.00211864406779661),\n",
       " (u'a', 0.01694915254237288),\n",
       " (u'\"yarn\"', 0.00211864406779661),\n",
       " (u'Thriftserver', 0.00211864406779661),\n",
       " (u'processing.', 0.00211864406779661),\n",
       " (u'./bin/spark-shell', 0.00211864406779661),\n",
       " (u'Python', 0.00423728813559322),\n",
       " (u'Spark](#building-spark).', 0.00211864406779661),\n",
       " (u'clean', 0.00211864406779661),\n",
       " (u'the', 0.046610169491525424),\n",
       " (u'requires', 0.00211864406779661),\n",
       " (u'talk', 0.00211864406779661),\n",
       " (u'help', 0.00211864406779661),\n",
       " (u'Hadoop', 0.006355932203389831),\n",
       " (u'-T', 0.00211864406779661),\n",
       " (u'high-level', 0.00211864406779661),\n",
       " (u'its', 0.00211864406779661),\n",
       " (u'web', 0.00211864406779661),\n",
       " (u'Shell', 0.00423728813559322),\n",
       " (u'how', 0.00423728813559322),\n",
       " (u'graph', 0.00211864406779661),\n",
       " (u'run:', 0.00211864406779661),\n",
       " (u'should', 0.00423728813559322),\n",
       " (u'to', 0.029661016949152543),\n",
       " (u'module,', 0.00211864406779661),\n",
       " (u'given.', 0.00211864406779661),\n",
       " (u'directory.', 0.00211864406779661),\n",
       " (u'must', 0.00211864406779661),\n",
       " (u'do', 0.00423728813559322),\n",
       " (u'Programs', 0.00211864406779661),\n",
       " (u'Many', 0.00211864406779661),\n",
       " (u'YARN,', 0.00211864406779661),\n",
       " (u'using', 0.01059322033898305),\n",
       " (u'Example', 0.00211864406779661),\n",
       " (u'Once', 0.00211864406779661),\n",
       " (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       "  0.00211864406779661),\n",
       " (u'Because', 0.00211864406779661),\n",
       " (u'name', 0.00211864406779661),\n",
       " (u'Testing', 0.00211864406779661),\n",
       " (u'refer', 0.00423728813559322),\n",
       " (u'Streaming', 0.00211864406779661),\n",
       " (u'[IntelliJ](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ).',\n",
       "  0.00211864406779661),\n",
       " (u'SQL', 0.00423728813559322),\n",
       " (u'them,', 0.00211864406779661),\n",
       " (u'analysis.', 0.00211864406779661),\n",
       " (u'set', 0.00423728813559322),\n",
       " (u'Scala', 0.00423728813559322),\n",
       " (u'thread,', 0.00211864406779661),\n",
       " (u'individual', 0.00211864406779661),\n",
       " (u'examples', 0.00423728813559322),\n",
       " (u'runs.', 0.00211864406779661),\n",
       " (u'Pi', 0.00211864406779661),\n",
       " (u'More', 0.00211864406779661),\n",
       " (u'Python,', 0.00423728813559322),\n",
       " (u'Versions', 0.00211864406779661),\n",
       " (u'find', 0.00211864406779661),\n",
       " (u'version', 0.00211864406779661),\n",
       " (u'wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n",
       "  0.00211864406779661),\n",
       " (u'`./bin/run-example', 0.00211864406779661),\n",
       " (u'Configuration', 0.00211864406779661),\n",
       " (u'command,', 0.00423728813559322),\n",
       " (u'Maven,', 0.00211864406779661),\n",
       " (u'core', 0.00211864406779661),\n",
       " (u'Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       "  0.00211864406779661),\n",
       " (u'MASTER=spark://host:7077', 0.00211864406779661),\n",
       " (u'Documentation', 0.00211864406779661),\n",
       " (u'downloaded', 0.00211864406779661),\n",
       " (u'distributions.', 0.00211864406779661),\n",
       " (u'Spark.', 0.00211864406779661),\n",
       " (u'[\"Building', 0.00211864406779661),\n",
       " (u'by', 0.00211864406779661),\n",
       " (u'on', 0.01059322033898305),\n",
       " (u'package', 0.00211864406779661),\n",
       " (u'of', 0.01059322033898305),\n",
       " (u'changed', 0.00211864406779661),\n",
       " (u'pre-built', 0.00211864406779661),\n",
       " (u'Big', 0.00211864406779661),\n",
       " (u'3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).',\n",
       "  0.00211864406779661),\n",
       " (u'or', 0.006355932203389831),\n",
       " (u'learning,', 0.00211864406779661),\n",
       " (u'locally', 0.00423728813559322),\n",
       " (u'overview', 0.00211864406779661),\n",
       " (u'one', 0.006355932203389831),\n",
       " (u'(You', 0.00211864406779661),\n",
       " (u'Online', 0.00211864406779661),\n",
       " (u'versions', 0.00211864406779661),\n",
       " (u'your', 0.00211864406779661),\n",
       " (u'threads.', 0.00211864406779661),\n",
       " (u'APIs', 0.00211864406779661),\n",
       " (u'SparkPi', 0.00423728813559322),\n",
       " (u'contains', 0.00211864406779661),\n",
       " (u'system', 0.00211864406779661),\n",
       " (u'`examples`', 0.00423728813559322),\n",
       " (u'start', 0.00211864406779661),\n",
       " (u'build/mvn', 0.00211864406779661),\n",
       " (u'easiest', 0.00211864406779661),\n",
       " (u'basic', 0.00211864406779661),\n",
       " (u'more', 0.00211864406779661),\n",
       " (u'option', 0.00211864406779661),\n",
       " (u'that', 0.00423728813559322),\n",
       " (u'N', 0.00211864406779661),\n",
       " (u'\"local[N]\"', 0.00211864406779661),\n",
       " (u'DataFrames,', 0.00211864406779661),\n",
       " (u'particular', 0.00423728813559322),\n",
       " (u'be', 0.00423728813559322),\n",
       " (u'an', 0.00847457627118644),\n",
       " (u'than', 0.00211864406779661),\n",
       " (u'Interactive', 0.00423728813559322),\n",
       " (u'builds', 0.00211864406779661),\n",
       " (u'developing', 0.00211864406779661),\n",
       " (u'programs', 0.00423728813559322),\n",
       " (u'cluster', 0.00423728813559322),\n",
       " (u'can', 0.014830508474576272),\n",
       " (u'example', 0.006355932203389831),\n",
       " (u'are', 0.00211864406779661),\n",
       " (u'Data.', 0.00211864406779661),\n",
       " (u'mesos://', 0.00211864406779661),\n",
       " (u'computing', 0.00211864406779661),\n",
       " (u'URL,', 0.00211864406779661),\n",
       " (u'in', 0.012711864406779662),\n",
       " (u'general', 0.00423728813559322),\n",
       " (u'To', 0.00423728813559322),\n",
       " (u'at', 0.00423728813559322),\n",
       " (u'1000).count()', 0.00211864406779661),\n",
       " (u'if', 0.00847457627118644),\n",
       " (u'built', 0.00211864406779661),\n",
       " (u'no', 0.00211864406779661),\n",
       " (u'Java,', 0.00211864406779661),\n",
       " (u'MLlib', 0.00211864406779661),\n",
       " (u'also', 0.00847457627118644),\n",
       " (u'other', 0.00211864406779661),\n",
       " (u'build', 0.00847457627118644),\n",
       " (u'online', 0.00211864406779661),\n",
       " (u'several', 0.00211864406779661),\n",
       " (u'HDFS', 0.00211864406779661),\n",
       " (u'[Configuration', 0.00211864406779661),\n",
       " (u'class', 0.00423728813559322),\n",
       " (u'>>>', 0.00211864406779661),\n",
       " (u'spark://', 0.00211864406779661),\n",
       " (u'page](http://spark.apache.org/documentation.html)', 0.00211864406779661),\n",
       " (u'documentation', 0.006355932203389831),\n",
       " (u'It', 0.00423728813559322),\n",
       " (u'graphs', 0.00211864406779661),\n",
       " (u'./dev/run-tests', 0.00211864406779661),\n",
       " (u'configure', 0.00211864406779661),\n",
       " (u'<class>', 0.00211864406779661),\n",
       " (u'first', 0.00211864406779661),\n",
       " (u'latest', 0.00211864406779661)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "wget -q -nc https://raw.githubusercontent.com/amitgroup/amitgroup/master/amitgroup/io/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### http://g.sweyla.com/blog/2012/mnist-numpy/\n",
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gzip: t10k-images-idx3-ubyte already exists;\tnot overwritten\n",
      "gzip: t10k-labels-idx1-ubyte already exists;\tnot overwritten\n",
      "gzip: train-images-idx3-ubyte already exists;\tnot overwritten\n",
      "gzip: train-labels-idx1-ubyte already exists;\tnot overwritten\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir -p mnist && {\n",
    "    cd mnist;\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz &&\n",
    "    wget -q -nc http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz &&\n",
    "    gunzip *.gz\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = mnist.load_mnist(dataset='training', path='mnist/')\n",
    "X = X.reshape(-1, 1, 28, 28)\n",
    "\n",
    "X_test, y_test = mnist.load_mnist(dataset='testing', path='mnist/')\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_rdd = sc.parallelize(\n",
    "    (i, y[i], X[i].ravel().copy())\n",
    "    for i in xrange(X.shape[0])\n",
    ").persist()\n",
    "\n",
    "X_test_rdd = sc.parallelize(\n",
    "    (i, y_test[i], X_test[i].ravel().copy())\n",
    "    for i in xrange(X_test.shape[0])\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_max = np.max(x)\n",
    "    \n",
    "    exps = np.exp(x - x_max)\n",
    "    \n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, C = 1.0e-3):\n",
    "        self.W = np.random.uniform(-1, 1, size=(28 * 28, 10))\n",
    "        self.b = np.random.uniform(-1, 1, size=(10, ))\n",
    "        \n",
    "        self.C = C\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ### note that this is not an efficient implementation\n",
    "        ### of X.dot(W)\n",
    "        return X.map(\n",
    "            lambda (i, y, x): softmax(x.dot(self.W) + self.b)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  2.02476371e-03,   9.60981585e-04,   8.13040289e-03,\n",
       "          3.17359036e-03,   1.49190365e-02,   4.81511129e-03,\n",
       "          1.08157937e-02,   9.42059854e-01,   1.30980772e-02,\n",
       "          2.38827921e-06]),\n",
       " array([  7.08920531e-01,   2.44384021e-06,   7.97656774e-05,\n",
       "          3.13705662e-08,   2.60347080e-08,   1.84057095e-11,\n",
       "          1.29229970e-01,   7.44069585e-06,   1.61757765e-01,\n",
       "          2.02621951e-06]),\n",
       " array([  9.66105450e-03,   8.79654741e-01,   1.41754681e-03,\n",
       "          6.57827614e-06,   7.38867021e-02,   1.85144117e-04,\n",
       "          1.70333515e-02,   3.25249027e-03,   1.15099660e-02,\n",
       "          3.39242565e-03]),\n",
       " array([  3.43460159e-02,   2.63254609e-03,   3.30914975e-01,\n",
       "          9.00755856e-07,   4.17104012e-03,   7.06484947e-02,\n",
       "          2.51425230e-01,   7.04200604e-02,   2.35392327e-01,\n",
       "          4.84103385e-05]),\n",
       " array([  2.80198154e-06,   2.71215256e-06,   6.77019650e-04,\n",
       "          6.12829718e-05,   2.52962946e-06,   2.71294854e-04,\n",
       "          4.93400426e-02,   2.28965541e-02,   9.26744682e-01,\n",
       "          1.07987688e-06])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_test_rdd).collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
